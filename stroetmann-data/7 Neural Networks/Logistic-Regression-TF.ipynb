{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows to do handwritten character recognition with logistic regression.\n",
    "I have adapted this example from an example of **Aymeric Damien**.  He has a lot of nice notebooks discussing TensorFlow at https://github.com/aymericdamien/TensorFlow-Examples/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{vectorized_result}(d)$ converts the digit $d \\in \\{0,\\cdots,9\\}$ and returns a NumPy vector $\\mathbf{x}$ of shape $(10, 1)$ such that\n",
    "$$\n",
    "\\mathbf{x}[i] = \n",
    "\\left\\{\n",
    "  \\begin{array}{ll}\n",
    "     1 & \\mbox{if $i = d$;} \\\\\n",
    "     0 & \\mbox{otherwise.}\n",
    "  \\end{array}  \n",
    "\\right.\n",
    "$$\n",
    "This function is used to convert a digit $d$ into the expected output of a neural network that has an output unit for every digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(d):\n",
    "    e    = np.zeros((10, ), dtype=np.float32)\n",
    "    e[d] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we are using is stored as a <a href=\"https://docs.python.org/3/library/gzip.html\">gzipped</a>, \n",
    "<a href=\"https://docs.python.org/3/library/pickle.html\">pickled</a> file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{load_data}()$ returns a pair of the form\n",
    "$$ (\\texttt{training_data}, \\texttt{test_data}) $$\n",
    "where \n",
    "- $\\texttt{training_data}$ is a list containing $50,000$ pairs $(\\textbf{x}, \\textbf{y})$ s.t. $\\textbf{x}$ is a \n",
    "  784-dimensional `numpy.ndarray` containing the input image and $\\textbf{y}$ is a 10-dimensional `numpy.ndarray`   \n",
    "  corresponding to the correct digit for x.\n",
    "- $\\texttt{test_data}$ is a list containing $10,000$ pairs $(\\textbf{x}, \\textbf{y})$.  In each case, \n",
    "  $\\textbf{x}$ is a 784-dimensional `numpy.ndarry` containing the input image\n",
    "  and $\\textbf{y}$ is a 10-dimensional `numpy.ndarray` corresponding to the correct digit for $\\textbf{x}$.\n",
    "  \n",
    "We do not use the validation data that are provided in the file `mnist.pkl.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        train, validate, test = pickle.load(f, encoding=\"latin1\")\n",
    "    X_train = np.array([np.reshape(x, (784, )) for x in train[0]])\n",
    "    X_test  = np.array([np.reshape(x, (784, )) for x in test [0]])\n",
    "    Y_train = np.array([vectorized_result(y) for y in train[1]])\n",
    "    Y_test  = np.array([vectorized_result(y) for y in test [1]])\n",
    "    return (X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{show_digit}(\\texttt{row}, \\texttt{columns}, \\texttt{offset})$ \n",
    "shows $\\texttt{row} \\cdot \\texttt{columns}$ images of the training data.  The first image shown is the image at index $\\texttt{offset}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digits(rows, columns, offset=0):\n",
    "    f, axarr = plt.subplots(rows, columns)\n",
    "    for r in range(rows):\n",
    "        for c in range(columns):\n",
    "            i     = r * columns + c + offset\n",
    "            image = 1 - X_train[i,:]\n",
    "            image = np.reshape(image, (28, 28))\n",
    "            axarr[r, c].imshow(image, cmap=\"gray\")\n",
    "            axarr[r, c].axis('off')\n",
    "    plt.savefig(\"digits.pdf\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_digits(5, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid a bug we have to set the following environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create <em style=\"color:blue\">placeholders</em> to use for the data.  Below, `None` stands for the yet unknown number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "Y = tf.placeholder(tf.float32, [None,  10]) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create <em style=\"color:blue\">variables</em> for the weights and biases.\n",
    "The variable `W` is the <em style=\"color:blue;\">weight matrix</em>, while `b` is the <em style=\"color:blue;\">bias vector</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the model for logistic regression. `Y_pred` is the prediction vector.  We use the \n",
    "<em style=\"color:blue;\">softmax activation function</em>.  For a $d$-dimensional vector $\\mathbf{z}$, this function is defined as\n",
    "$$ \\sigma(\\mathbf{z})_i := \\frac{\\exp(z_i)}{\\;\\displaystyle\\sum\\limits_{j=1}^d \\exp(z_j)\\;} $$\n",
    "This function is predifined in TensorFlow.\n",
    "Here, the vector $\\mathbf{z}$ is defined as\n",
    "$$ \\mathbf{z} = \\mathbf{x} \\cdot W + \\mathbf{b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.nn.softmax(tf.matmul(X, W) + b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the <em style=\"color:blue;\">cross entropy</em> as a cost function. This is defined as\n",
    "$$ -\\sum\\limits_{i=1}^d \\mathtt{Y}_i \\cdot \\ln(\\mathtt{Y\\_pred}_i) $$\n",
    "Here, $\\mathtt{Y}_i$ is the expected outcome, while $\\mathtt{Y\\_pred}_i$ is the output predicted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_pred), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some <em style=\"color:blue;\">hyperparameters</em>.  We will use \n",
    "<em style=\"color:blue;\">stochastic gradient descent</em> with a minibatch size of $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.05\n",
    "training_epochs = 50\n",
    "batch_size      = 100\n",
    "num_examples    = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use <em style=\"color:blue;\">stochastic gradient descent</em> to minimize this cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{next_batch}(s)$ returns the next batch of the size $s$.  It returns a pair \n",
    "of the form $(X, Y)$ where $X$ is a matrix of shape $(s, 784)$ and $Y$ is a matrix of \n",
    "shape $(s, 10)$.  The function updates the global variable `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(size):\n",
    "    global count\n",
    "    X_batch  = X_train[count:count+size,:]\n",
    "    Y_batch  = Y_train[count:count+size,:]\n",
    "    count   += size\n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as tfs:\n",
    "    tfs.run(init)\n",
    "    for epoch in range(training_epochs): \n",
    "        count = 0\n",
    "        avg_cost = 0.0\n",
    "        num_batches = int(num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(num_batches):\n",
    "            X_batch, Y_batch = next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = tfs.run([optimizer, cost], {X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / num_batches\n",
    "        print(\"Epoch:\", '%2d,' % epoch, \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Test model\n",
    "    correct = tfs.run(tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y, 1)), {X: X_test, Y: Y_test})\n",
    "\n",
    "print(\"Accuracy:\", np.sum(correct) / len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
