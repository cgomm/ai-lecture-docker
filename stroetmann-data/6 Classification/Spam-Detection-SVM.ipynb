{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100%; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 0\n",
    "from IPython.core.display import HTML, display\n",
    "display(HTML('<style>.container { width:100%; } </style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection  Using a Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating a spam detector using a Support Vector Machine is split up into five steps.\n",
    "\n",
    "  - Create a set of the most common words occurring in spam and ham (i.e. non-spam) emails.\n",
    "  - Transform every mail into a <em style=\"color:blue\">frequency vector</em>: For every word in the set of most common words, \n",
    "    the frequency vector stores the frequency of this word in the respective mail.\n",
    "  - For every word in the list of most common word, compute the <em style=\"color:blue\">inverse document frequency</em>.\n",
    "  - Compute the <em style=\"color:blue\">feature matrix</em> by transforming the frequency vectors into vectors that contain the product of the \n",
    "    <em style=\"color:blue\">term frequency</em> with the <em style=\"color:blue\">inverse document frequency</em>.\n",
    "  - Train and test an SVM using this <em style=\"color:blue\">feature matrix</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Set of Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory \n",
    "https://github.com/karlstroetmann/Artificial-Intelligence/tree/master/Python/EmailData\n",
    "contains 960 emails that are divided into four subdirectories:\n",
    "\n",
    "  - `spam-train` contains 350 spam emails for training,\n",
    "  - `ham-train`  contains 350 non-spam emails for training,\n",
    "  - `spam-test`  contains 130 spam emails for testing,\n",
    "  - `ham-test`   contains 130 non-spam emails for testing.\n",
    "\n",
    "I have found this data on the page \n",
    "http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html provided by Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare some variables so that this notebook can be adapted to other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir_train = 'EmailData/spam-train/'\n",
    "ham__dir_train = 'EmailData/ham-train/'\n",
    "spam_dir_test  = 'EmailData/spam-test/'\n",
    "ham__dir_test  = 'EmailData/ham-test/'\n",
    "Directories    = [spam_dir_train, ham__dir_train, spam_dir_test, ham__dir_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{get_word_set}(\\texttt{fn})$ takes a filename $\\texttt{fn}$ as its argument.  It reads the file and returns a `set` of all words that are found in this file.  The words are transformed to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_set(fn):\n",
    "    with open(fn) as file:\n",
    "        text = file.read()\n",
    "        text = text.lower()\n",
    "        return set(re.findall(r\"[\\w']+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_all_files` reads all files contained in those directories that are stored in the list `Directories`. \n",
    "It returns a `Counter`.  For every word $w$ this counter contains the number of files that contain $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files():\n",
    "    Words = Counter()\n",
    "    for directory in Directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            Words.update(get_words_set(directory + file_name))\n",
    "    return Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Common_Words` is a `numpy` array of the 2500 most common words found in all of our emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M            = 2500             # number of the most common words to use\n",
    "Word_Counter = read_all_files()\n",
    "Common_Words = np.array(list({ w for w, _ in Word_Counter.most_common(M) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hundr', 'handbook', 'san', ..., 'friday', 'instructions', 'act'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Common_Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transform Files into Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Index_Dict` is a dictionary that maps from the most common words to their index in the array `Common_Words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hundr': 0,\n",
       " 'handbook': 1,\n",
       " 'san': 2,\n",
       " 'imagination': 3,\n",
       " 'pass': 4,\n",
       " 'effectively': 5,\n",
       " 'television': 6,\n",
       " 'generative': 7,\n",
       " 'initial': 8,\n",
       " 'enhance': 9,\n",
       " 'edu': 10,\n",
       " 'university': 11,\n",
       " 'system': 12,\n",
       " 'post': 13,\n",
       " 'society': 14,\n",
       " 'fully': 15,\n",
       " 'comprehensive': 16,\n",
       " 'frequently': 17,\n",
       " 'bank': 18,\n",
       " 'mediumsize': 19,\n",
       " 'count': 20,\n",
       " 'extensive': 21,\n",
       " 'marketer': 22,\n",
       " 'excess': 23,\n",
       " 'strongly': 24,\n",
       " 'constraint': 25,\n",
       " 'convince': 26,\n",
       " 'clause': 27,\n",
       " 'western': 28,\n",
       " 'blank': 29,\n",
       " 'hardware': 30,\n",
       " 'quebec': 31,\n",
       " 'motivate': 32,\n",
       " 'al': 33,\n",
       " 'ourselve': 34,\n",
       " 'consult': 35,\n",
       " 'confidential': 36,\n",
       " 'function': 37,\n",
       " 'west': 38,\n",
       " 'order': 39,\n",
       " 'cloth': 40,\n",
       " 'concern': 41,\n",
       " 'property': 42,\n",
       " 'entitle': 43,\n",
       " 'greek': 44,\n",
       " 'explore': 45,\n",
       " 'mind': 46,\n",
       " 'application': 47,\n",
       " 'limit': 48,\n",
       " 'institut': 49,\n",
       " 'cognitive': 50,\n",
       " 'intrusion': 51,\n",
       " 'finance': 52,\n",
       " 'parttime': 53,\n",
       " 'morpheme': 54,\n",
       " 'water': 55,\n",
       " 'attention': 56,\n",
       " 'forum': 57,\n",
       " 'revenue': 58,\n",
       " 'belgium': 59,\n",
       " 'corpus': 60,\n",
       " 'houston': 61,\n",
       " 'abstract': 62,\n",
       " 'determine': 63,\n",
       " 'since': 64,\n",
       " 'hours': 65,\n",
       " 'representation': 66,\n",
       " 'respond': 67,\n",
       " 'likely': 68,\n",
       " 'html': 69,\n",
       " 'italian': 70,\n",
       " 'reserve': 71,\n",
       " 'media': 72,\n",
       " 'longer': 73,\n",
       " 'header': 74,\n",
       " 'progress': 75,\n",
       " 'consideration': 76,\n",
       " 'specialist': 77,\n",
       " 'seven': 78,\n",
       " 'call': 79,\n",
       " 'competition': 80,\n",
       " 'mo': 81,\n",
       " 'educational': 82,\n",
       " 'aim': 83,\n",
       " 'capitalfm': 84,\n",
       " 'ignore': 85,\n",
       " 'info': 86,\n",
       " 'summarize': 87,\n",
       " 'austin': 88,\n",
       " 'old': 89,\n",
       " 'client': 90,\n",
       " 'advance': 91,\n",
       " 'female': 92,\n",
       " 'mexico': 93,\n",
       " 'hbe': 94,\n",
       " 'underlie': 95,\n",
       " 'im': 96,\n",
       " 'n': 97,\n",
       " 'maximum': 98,\n",
       " 'visit': 99,\n",
       " 'science': 100,\n",
       " 'weekly': 101,\n",
       " 'growth': 102,\n",
       " 'florida': 103,\n",
       " 'md': 104,\n",
       " 'diploma': 105,\n",
       " 'business': 106,\n",
       " 'aspect': 107,\n",
       " 'edward': 108,\n",
       " 'extremely': 109,\n",
       " 'verify': 110,\n",
       " 'card': 111,\n",
       " 'ram': 112,\n",
       " 'conduct': 113,\n",
       " 'practice': 114,\n",
       " 'mci': 115,\n",
       " 'law': 116,\n",
       " 'enquiry': 117,\n",
       " 'march': 118,\n",
       " 'fl': 119,\n",
       " 'compute': 120,\n",
       " 'driver': 121,\n",
       " 'online': 122,\n",
       " 'travel': 123,\n",
       " 'services': 124,\n",
       " 'replace': 125,\n",
       " 'discovery': 126,\n",
       " 'christian': 127,\n",
       " 'organise': 128,\n",
       " 'juno': 129,\n",
       " 'compatible': 130,\n",
       " 'disk': 131,\n",
       " 'cannot': 132,\n",
       " 'recent': 133,\n",
       " 'space': 134,\n",
       " 'ever': 135,\n",
       " 'amex': 136,\n",
       " 'empirical': 137,\n",
       " 'restriction': 138,\n",
       " 'key': 139,\n",
       " 'basis': 140,\n",
       " 'sociolinguistic': 141,\n",
       " 'concept': 142,\n",
       " 'most': 143,\n",
       " 'sexually': 144,\n",
       " 'rock': 145,\n",
       " 'judgment': 146,\n",
       " 'mr': 147,\n",
       " 'article': 148,\n",
       " 'rom': 149,\n",
       " 'photo': 150,\n",
       " 'arrange': 151,\n",
       " 'excite': 152,\n",
       " 'everythe': 153,\n",
       " 'five': 154,\n",
       " 'presence': 155,\n",
       " 'attract': 156,\n",
       " 'amaze': 157,\n",
       " 'update': 158,\n",
       " 'forward': 159,\n",
       " 'york': 160,\n",
       " 'prospective': 161,\n",
       " 'hundred': 162,\n",
       " 'homebase': 163,\n",
       " 'piece': 164,\n",
       " 'upgrade': 165,\n",
       " 'guest': 166,\n",
       " 'even': 167,\n",
       " 'chat': 168,\n",
       " 'kind': 169,\n",
       " 'xxx': 170,\n",
       " 'profitable': 171,\n",
       " 'spout': 172,\n",
       " 'forthcome': 173,\n",
       " 'display': 174,\n",
       " 'exp': 175,\n",
       " 'list': 176,\n",
       " 'sense': 177,\n",
       " 'morphological': 178,\n",
       " 'writer': 179,\n",
       " 'generally': 180,\n",
       " 'traffic': 181,\n",
       " 'sery': 182,\n",
       " 'day': 183,\n",
       " 'collect': 184,\n",
       " 'lists': 185,\n",
       " 'ed': 186,\n",
       " 'faster': 187,\n",
       " 'vary': 188,\n",
       " 'agent': 189,\n",
       " 'der': 190,\n",
       " 'debt': 191,\n",
       " 'category': 192,\n",
       " 'non': 193,\n",
       " 'proper': 194,\n",
       " 'foreign': 195,\n",
       " 'wherea': 196,\n",
       " 'meet': 197,\n",
       " 'canadian': 198,\n",
       " 'mortgage': 199,\n",
       " 'ad': 200,\n",
       " 'factor': 201,\n",
       " 'fantastic': 202,\n",
       " 'player': 203,\n",
       " 'flame': 204,\n",
       " 'city': 205,\n",
       " 'musical': 206,\n",
       " 'sue': 207,\n",
       " 'reward': 208,\n",
       " 'mac': 209,\n",
       " 'remark': 210,\n",
       " 'estate': 211,\n",
       " 'represent': 212,\n",
       " 'stage': 213,\n",
       " 'eventually': 214,\n",
       " 'green': 215,\n",
       " 'device': 216,\n",
       " 'major': 217,\n",
       " 'interactive': 218,\n",
       " 'government': 219,\n",
       " 'query': 220,\n",
       " 'coordinator': 221,\n",
       " 'hello': 222,\n",
       " 'prefer': 223,\n",
       " 'position': 224,\n",
       " 'immediate': 225,\n",
       " 'lose': 226,\n",
       " 'quite': 227,\n",
       " 'blvd': 228,\n",
       " 'venue': 229,\n",
       " 'favourite': 230,\n",
       " 'still': 231,\n",
       " 'minimal': 232,\n",
       " 'germanic': 233,\n",
       " 'ongo': 234,\n",
       " 'case': 235,\n",
       " 'college': 236,\n",
       " 'comparative': 237,\n",
       " 'professor': 238,\n",
       " 'newsletter': 239,\n",
       " 'anonymous': 240,\n",
       " 'noun': 241,\n",
       " 'america': 242,\n",
       " 'seminar': 243,\n",
       " 'harri': 244,\n",
       " 'advertise': 245,\n",
       " 'dozen': 246,\n",
       " 'hottest': 247,\n",
       " 'novel': 248,\n",
       " 'l': 249,\n",
       " 'residual': 250,\n",
       " 'un': 251,\n",
       " 'etc': 252,\n",
       " 'student': 253,\n",
       " 'returns': 254,\n",
       " 'was': 255,\n",
       " 'log': 256,\n",
       " 'conclusion': 257,\n",
       " 'serious': 258,\n",
       " 'entertainment': 259,\n",
       " 'memory': 260,\n",
       " 'planck': 261,\n",
       " 'sound': 262,\n",
       " 'word': 263,\n",
       " 'ba': 264,\n",
       " 'bear': 265,\n",
       " 'jame': 266,\n",
       " 'rush': 267,\n",
       " 'really': 268,\n",
       " 'happen': 269,\n",
       " 'pick': 270,\n",
       " 'sort': 271,\n",
       " 'macintosh': 272,\n",
       " 'detail': 273,\n",
       " 'theory': 274,\n",
       " 'african': 275,\n",
       " 'least': 276,\n",
       " 'us': 277,\n",
       " 'indefinite': 278,\n",
       " 'ensure': 279,\n",
       " 'station': 280,\n",
       " 'pic': 281,\n",
       " 'convenience': 282,\n",
       " 'phonetic': 283,\n",
       " 'du': 284,\n",
       " 'addresses': 285,\n",
       " 'engineer': 286,\n",
       " 'responsibility': 287,\n",
       " 'first': 288,\n",
       " 'indicate': 289,\n",
       " 'speakers': 290,\n",
       " 'response': 291,\n",
       " 'persistent': 292,\n",
       " 'papers': 293,\n",
       " 'acquire': 294,\n",
       " 'discount': 295,\n",
       " 'fact': 296,\n",
       " 'id': 297,\n",
       " 'practically': 298,\n",
       " 'personality': 299,\n",
       " 'contain': 300,\n",
       " 'increase': 301,\n",
       " 'six': 302,\n",
       " 'price': 303,\n",
       " 'seek': 304,\n",
       " 'intention': 305,\n",
       " 'fast': 306,\n",
       " 'linguistic': 307,\n",
       " 'oral': 308,\n",
       " 'payable': 309,\n",
       " 'distinction': 310,\n",
       " 'tips': 311,\n",
       " 'works': 312,\n",
       " 'among': 313,\n",
       " 'protect': 314,\n",
       " 'performance': 315,\n",
       " 'delay': 316,\n",
       " 'september': 317,\n",
       " 'few': 318,\n",
       " 'decade': 319,\n",
       " 'structure': 320,\n",
       " 'verb': 321,\n",
       " 'obtain': 322,\n",
       " 'identification': 323,\n",
       " 'doe': 324,\n",
       " 'capability': 325,\n",
       " 'perception': 326,\n",
       " 'present': 327,\n",
       " 'relationship': 328,\n",
       " 'scholar': 329,\n",
       " 'must': 330,\n",
       " 'bruce': 331,\n",
       " 'xerox': 332,\n",
       " 'cmu': 333,\n",
       " 'jeff': 334,\n",
       " 'describe': 335,\n",
       " 'paid': 336,\n",
       " 'surely': 337,\n",
       " 'biz': 338,\n",
       " 'advertiser': 339,\n",
       " 'methodology': 340,\n",
       " 'vium': 341,\n",
       " 'programme': 342,\n",
       " 'learn': 343,\n",
       " 'v': 344,\n",
       " 'significant': 345,\n",
       " 'greatest': 346,\n",
       " 'search': 347,\n",
       " 'emerge': 348,\n",
       " 'spain': 349,\n",
       " 'microsoft': 350,\n",
       " 'reports': 351,\n",
       " 'east': 352,\n",
       " 'arabic': 353,\n",
       " 'positive': 354,\n",
       " 'criterion': 355,\n",
       " 'cut': 356,\n",
       " 'wide': 357,\n",
       " 'goods': 358,\n",
       " 'graphic': 359,\n",
       " 'especially': 360,\n",
       " 'technical': 361,\n",
       " 'electronic': 362,\n",
       " 'laboratory': 363,\n",
       " 'produce': 364,\n",
       " 'nc': 365,\n",
       " 'hi': 366,\n",
       " 'video': 367,\n",
       " 'poster': 368,\n",
       " 'apply': 369,\n",
       " 'appeal': 370,\n",
       " 'professional': 371,\n",
       " 'asset': 372,\n",
       " 'lunch': 373,\n",
       " 'faculty': 374,\n",
       " 'bag': 375,\n",
       " 'highway': 376,\n",
       " 'van': 377,\n",
       " 'd': 378,\n",
       " 'hold': 379,\n",
       " 'ca': 380,\n",
       " 'generate': 381,\n",
       " 'karen': 382,\n",
       " 'scott': 383,\n",
       " 'luck': 384,\n",
       " 'bid': 385,\n",
       " 'seller': 386,\n",
       " 'past': 387,\n",
       " 'administration': 388,\n",
       " 'access': 389,\n",
       " 'chance': 390,\n",
       " 'anne': 391,\n",
       " 'bibliography': 392,\n",
       " 'newsgroup': 393,\n",
       " 'environment': 394,\n",
       " 'reservation': 395,\n",
       " 'ascii': 396,\n",
       " 'storm': 397,\n",
       " 'doctor': 398,\n",
       " 'john': 399,\n",
       " 'imply': 400,\n",
       " 'hop': 401,\n",
       " 'speaker': 402,\n",
       " 'premier': 403,\n",
       " 'opinion': 404,\n",
       " 'nj': 405,\n",
       " 'ext': 406,\n",
       " 'hardcopy': 407,\n",
       " 'sale': 408,\n",
       " 'school': 409,\n",
       " 'specific': 410,\n",
       " 'subscription': 411,\n",
       " 'keyword': 412,\n",
       " 'camera': 413,\n",
       " 'solve': 414,\n",
       " 'contents': 415,\n",
       " 'ours': 416,\n",
       " 'additional': 417,\n",
       " 'equivalent': 418,\n",
       " 'manchester': 419,\n",
       " 'production': 420,\n",
       " 'shop': 421,\n",
       " 'msn': 422,\n",
       " 'ibm': 423,\n",
       " 'registration': 424,\n",
       " 'population': 425,\n",
       " 'observation': 426,\n",
       " 'mailing': 427,\n",
       " 'postal': 428,\n",
       " 'however': 429,\n",
       " 'zero': 430,\n",
       " 'beach': 431,\n",
       " 'food': 432,\n",
       " 'context': 433,\n",
       " 'onto': 434,\n",
       " 'discussion': 435,\n",
       " 'financial': 436,\n",
       " 'powerful': 437,\n",
       " 'place': 438,\n",
       " 'fund': 439,\n",
       " 'sorry': 440,\n",
       " 'mode': 441,\n",
       " 'junk': 442,\n",
       " 'rights': 443,\n",
       " 'benefits': 444,\n",
       " 'spend': 445,\n",
       " 'jone': 446,\n",
       " 'expression': 447,\n",
       " 'fourth': 448,\n",
       " 'extent': 449,\n",
       " 'audience': 450,\n",
       " 'lower': 451,\n",
       " 'london': 452,\n",
       " 'easily': 453,\n",
       " 'why': 454,\n",
       " 'largest': 455,\n",
       " 'step': 456,\n",
       " 'model': 457,\n",
       " 'clean': 458,\n",
       " 'mention': 459,\n",
       " 'girl': 460,\n",
       " 'fall': 461,\n",
       " 'skeptical': 462,\n",
       " 'education': 463,\n",
       " 'confident': 464,\n",
       " 'russian': 465,\n",
       " 'british': 466,\n",
       " 'code': 467,\n",
       " 'set': 468,\n",
       " 'puzzle': 469,\n",
       " 'assumption': 470,\n",
       " 'fortune': 471,\n",
       " 'brian': 472,\n",
       " 'harvard': 473,\n",
       " 'coordinate': 474,\n",
       " 'pp': 475,\n",
       " 'thank': 476,\n",
       " 'latest': 477,\n",
       " 'loui': 478,\n",
       " 'britain': 479,\n",
       " 'wonder': 480,\n",
       " 'nijmegen': 481,\n",
       " 'hide': 482,\n",
       " 'germany': 483,\n",
       " 'region': 484,\n",
       " 'boyfriend': 485,\n",
       " 'demand': 486,\n",
       " 'along': 487,\n",
       " 'lie': 488,\n",
       " 'area': 489,\n",
       " 'firm': 490,\n",
       " 'automatic': 491,\n",
       " 'stuttgart': 492,\n",
       " 'assistant': 493,\n",
       " 'hypothesis': 494,\n",
       " 'phonetics': 495,\n",
       " 'owe': 496,\n",
       " 'onetime': 497,\n",
       " 'relevant': 498,\n",
       " 'planet': 499,\n",
       " 'reader': 500,\n",
       " 'umontreal': 501,\n",
       " 'believer': 502,\n",
       " 'line': 503,\n",
       " 'billion': 504,\n",
       " 'personally': 505,\n",
       " 'put': 506,\n",
       " 'operation': 507,\n",
       " 'representative': 508,\n",
       " 'unit': 509,\n",
       " 'cdrom': 510,\n",
       " 'contribution': 511,\n",
       " 'nothing': 512,\n",
       " 'envelope': 513,\n",
       " 'statement': 514,\n",
       " 'often': 515,\n",
       " 'thompson': 516,\n",
       " 'relax': 517,\n",
       " 'phone': 518,\n",
       " 'refinance': 519,\n",
       " 'monday': 520,\n",
       " 'korean': 521,\n",
       " 'inc': 522,\n",
       " 'moreover': 523,\n",
       " 'secure': 524,\n",
       " 'facility': 525,\n",
       " 'usually': 526,\n",
       " 'little': 527,\n",
       " 'thought': 528,\n",
       " 'dollars': 529,\n",
       " 'experiment': 530,\n",
       " 'charle': 531,\n",
       " 'proceedings': 532,\n",
       " 'quantity': 533,\n",
       " 'artificial': 534,\n",
       " 'random': 535,\n",
       " 'colingacl': 536,\n",
       " 'cable': 537,\n",
       " 'th': 538,\n",
       " 'ac': 539,\n",
       " 'instead': 540,\n",
       " 'gamble': 541,\n",
       " 'steven': 542,\n",
       " 'fee': 543,\n",
       " 'broadcast': 544,\n",
       " 'communication': 545,\n",
       " 'deadline': 546,\n",
       " 'seem': 547,\n",
       " 'made': 548,\n",
       " 'expensive': 549,\n",
       " 'plural': 550,\n",
       " 'background': 551,\n",
       " 'genie': 552,\n",
       " 'world': 553,\n",
       " 'rest': 554,\n",
       " 'means': 555,\n",
       " 'color': 556,\n",
       " 'cv': 557,\n",
       " 'nobody': 558,\n",
       " 'spokane': 559,\n",
       " 'air': 560,\n",
       " 'overt': 561,\n",
       " 'approve': 562,\n",
       " 'catch': 563,\n",
       " 'derive': 564,\n",
       " 'fresh': 565,\n",
       " 'native': 566,\n",
       " 'accepted': 567,\n",
       " 'boss': 568,\n",
       " 'conversational': 569,\n",
       " 'paradise': 570,\n",
       " 'guarantee': 571,\n",
       " 'whole': 572,\n",
       " 'man': 573,\n",
       " 'statistical': 574,\n",
       " 'prompt': 575,\n",
       " 'start': 576,\n",
       " 'frank': 577,\n",
       " 'correct': 578,\n",
       " 'centre': 579,\n",
       " 'cm': 580,\n",
       " 'install': 581,\n",
       " 'delivery': 582,\n",
       " 'listen': 583,\n",
       " 'zone': 584,\n",
       " 'jump': 585,\n",
       " 'between': 586,\n",
       " 'social': 587,\n",
       " 'interface': 588,\n",
       " 'tune': 589,\n",
       " 'december': 590,\n",
       " 'celebrate': 591,\n",
       " 'corporations': 592,\n",
       " 'importance': 593,\n",
       " 'distinguish': 594,\n",
       " 'shall': 595,\n",
       " 'pennsylvanium': 596,\n",
       " 'country': 597,\n",
       " 'speed': 598,\n",
       " 'dori': 599,\n",
       " 'team': 600,\n",
       " 'european': 601,\n",
       " 'picture': 602,\n",
       " 'stuff': 603,\n",
       " 'teach': 604,\n",
       " 'cfp': 605,\n",
       " 'company': 606,\n",
       " 'url': 607,\n",
       " 'historical': 608,\n",
       " 'invest': 609,\n",
       " 'either': 610,\n",
       " 'those': 611,\n",
       " 'intelligent': 612,\n",
       " 'our': 613,\n",
       " 'inch': 614,\n",
       " 'home': 615,\n",
       " 'legitimate': 616,\n",
       " 'slavic': 617,\n",
       " 'contrastive': 618,\n",
       " 'staff': 619,\n",
       " 'assessment': 620,\n",
       " 'scientific': 621,\n",
       " 'participant': 622,\n",
       " 'button': 623,\n",
       " 'dialogue': 624,\n",
       " 'enterprise': 625,\n",
       " 'gay': 626,\n",
       " 'discover': 627,\n",
       " 'stun': 628,\n",
       " 'sprachwissenschaft': 629,\n",
       " 'propose': 630,\n",
       " 'synthesis': 631,\n",
       " 'late': 632,\n",
       " 'awhile': 633,\n",
       " 'war': 634,\n",
       " 'state': 635,\n",
       " 'package': 636,\n",
       " 'blackwell': 637,\n",
       " 'newest': 638,\n",
       " 'match': 639,\n",
       " 'lead': 640,\n",
       " 'japanese': 641,\n",
       " 'official': 642,\n",
       " 'celebrity': 643,\n",
       " 'french': 644,\n",
       " 'publisher': 645,\n",
       " 'near': 646,\n",
       " 'font': 647,\n",
       " 'art': 648,\n",
       " 'train': 649,\n",
       " 'reject': 650,\n",
       " 'thus': 651,\n",
       " 'answer': 652,\n",
       " 'user': 653,\n",
       " 'familiar': 654,\n",
       " 'too': 655,\n",
       " 'draft': 656,\n",
       " 'expense': 657,\n",
       " 've': 658,\n",
       " 'mclaughlin': 659,\n",
       " 'consonant': 660,\n",
       " 'secondary': 661,\n",
       " 'notify': 662,\n",
       " 'ann': 663,\n",
       " 'worth': 664,\n",
       " 'saturday': 665,\n",
       " 'round': 666,\n",
       " 'total': 667,\n",
       " 'much': 668,\n",
       " 'suggestion': 669,\n",
       " 'ohio': 670,\n",
       " 'black': 671,\n",
       " 'wh': 672,\n",
       " 'walter': 673,\n",
       " 'stamp': 674,\n",
       " 'logic': 675,\n",
       " 'ny': 676,\n",
       " 'foot': 677,\n",
       " 'embark': 678,\n",
       " 'preference': 679,\n",
       " 'morn': 680,\n",
       " 'joseph': 681,\n",
       " 'likewise': 682,\n",
       " 'activity': 683,\n",
       " 'register': 684,\n",
       " 'store': 685,\n",
       " 'typological': 686,\n",
       " 'disc': 687,\n",
       " 'alter': 688,\n",
       " 'pound': 689,\n",
       " 'translate': 690,\n",
       " 'lawful': 691,\n",
       " 'did': 692,\n",
       " 'adult': 693,\n",
       " 'doubt': 694,\n",
       " 'subscribe': 695,\n",
       " 'virtual': 696,\n",
       " 'development': 697,\n",
       " 'speech': 698,\n",
       " 'cite': 699,\n",
       " 'sean': 700,\n",
       " 'max': 701,\n",
       " 'washington': 702,\n",
       " 'support': 703,\n",
       " 'goal': 704,\n",
       " 'californium': 705,\n",
       " 'cost': 706,\n",
       " 'alway': 707,\n",
       " 'batch': 708,\n",
       " 'il': 709,\n",
       " 'content': 710,\n",
       " 'ma': 711,\n",
       " 'male': 712,\n",
       " 'permanently': 713,\n",
       " 'amateur': 714,\n",
       " 'wife': 715,\n",
       " 'structural': 716,\n",
       " 'accessible': 717,\n",
       " 'computational': 718,\n",
       " 'term': 719,\n",
       " 'extractor': 720,\n",
       " 'head': 721,\n",
       " 'integration': 722,\n",
       " 'studies': 723,\n",
       " 'dramatically': 724,\n",
       " 'utterance': 725,\n",
       " 'latex': 726,\n",
       " 'merciless': 727,\n",
       " 'profile': 728,\n",
       " 'illustrate': 729,\n",
       " 'bed': 730,\n",
       " 'dare': 731,\n",
       " 'strength': 732,\n",
       " 'dure': 733,\n",
       " 'integrate': 734,\n",
       " 'perfectly': 735,\n",
       " 'myself': 736,\n",
       " 'die': 737,\n",
       " 'enable': 738,\n",
       " 'robbery': 739,\n",
       " 'market': 740,\n",
       " 'sexual': 741,\n",
       " 'primary': 742,\n",
       " 'furthermore': 743,\n",
       " 'consider': 744,\n",
       " 'achievement': 745,\n",
       " 'component': 746,\n",
       " 'pragmatic': 747,\n",
       " 'write': 748,\n",
       " 'privacy': 749,\n",
       " 'profit': 750,\n",
       " 'stealth': 751,\n",
       " 'pure': 752,\n",
       " 'item': 753,\n",
       " 'nd': 754,\n",
       " 'convention': 755,\n",
       " 'robert': 756,\n",
       " 'accept': 757,\n",
       " 'weekend': 758,\n",
       " 'cum': 759,\n",
       " 'advantage': 760,\n",
       " 'retrieval': 761,\n",
       " 'here': 762,\n",
       " 'benjamin': 763,\n",
       " 'christopher': 764,\n",
       " 'tom': 765,\n",
       " 'film': 766,\n",
       " 'bulk': 767,\n",
       " 'role': 768,\n",
       " 'movie': 769,\n",
       " 'field': 770,\n",
       " 'successful': 771,\n",
       " 'omit': 772,\n",
       " 'anthony': 773,\n",
       " 'att': 774,\n",
       " 'off': 775,\n",
       " 'address': 776,\n",
       " 'lanse': 777,\n",
       " 'grateful': 778,\n",
       " 'service': 779,\n",
       " 'fill': 780,\n",
       " 'coffee': 781,\n",
       " 'applicant': 782,\n",
       " 'begin': 783,\n",
       " 'listing': 784,\n",
       " 'organiser': 785,\n",
       " 'every': 786,\n",
       " 'txt': 787,\n",
       " 'wait': 788,\n",
       " 'fairchild': 789,\n",
       " 'rise': 790,\n",
       " 'vision': 791,\n",
       " 'pm': 792,\n",
       " 'expiration': 793,\n",
       " 'phillip': 794,\n",
       " 'nice': 795,\n",
       " 'channel': 796,\n",
       " 'mit': 797,\n",
       " 'angele': 798,\n",
       " 'currency': 799,\n",
       " 'break': 800,\n",
       " 'dependency': 801,\n",
       " 'morphology': 802,\n",
       " 'sake': 803,\n",
       " 'candidate': 804,\n",
       " 'finger': 805,\n",
       " 'stop': 806,\n",
       " 'htm': 807,\n",
       " 'adopt': 808,\n",
       " 'useful': 809,\n",
       " 'membership': 810,\n",
       " 'specifically': 811,\n",
       " 'sent': 812,\n",
       " 'psychology': 813,\n",
       " 'fm': 814,\n",
       " 'trash': 815,\n",
       " 'entirely': 816,\n",
       " 'enter': 817,\n",
       " 'gov': 818,\n",
       " 'choose': 819,\n",
       " 'almost': 820,\n",
       " 'tx': 821,\n",
       " 'schedule': 822,\n",
       " 'ahead': 823,\n",
       " 'tv': 824,\n",
       " 'nearly': 825,\n",
       " 'private': 826,\n",
       " 'phd': 827,\n",
       " 'retire': 828,\n",
       " 'study': 829,\n",
       " 'chomsky': 830,\n",
       " 'various': 831,\n",
       " 'plans': 832,\n",
       " 'waste': 833,\n",
       " 'proud': 834,\n",
       " 'ordering': 835,\n",
       " 'self': 836,\n",
       " 'description': 837,\n",
       " 'november': 838,\n",
       " 'loan': 839,\n",
       " 'quickly': 840,\n",
       " 'requirement': 841,\n",
       " 'corner': 842,\n",
       " 'responsible': 843,\n",
       " 'prove': 844,\n",
       " 'europe': 845,\n",
       " 'roll': 846,\n",
       " 'nature': 847,\n",
       " 'hand': 848,\n",
       " 'remove': 849,\n",
       " 'pittsburgh': 850,\n",
       " 'accomodation': 851,\n",
       " 'paste': 852,\n",
       " 'hundreds': 853,\n",
       " 'addition': 854,\n",
       " 'a': 855,\n",
       " 'behalf': 856,\n",
       " 'health': 857,\n",
       " 'teacher': 858,\n",
       " 'classroom': 859,\n",
       " 'gold': 860,\n",
       " 'william': 861,\n",
       " 'oversea': 862,\n",
       " 'dream': 863,\n",
       " 'http': 864,\n",
       " 'hour': 865,\n",
       " 'download': 866,\n",
       " 'occur': 867,\n",
       " 'continual': 868,\n",
       " 'topic': 869,\n",
       " 'free': 870,\n",
       " 'predicate': 871,\n",
       " 'exactly': 872,\n",
       " 'released': 873,\n",
       " 'contract': 874,\n",
       " 'unlimited': 875,\n",
       " 'toll': 876,\n",
       " 'approximately': 877,\n",
       " 'exceed': 878,\n",
       " 'employ': 879,\n",
       " 'engage': 880,\n",
       " 'universitaet': 881,\n",
       " 'pro': 882,\n",
       " 'summer': 883,\n",
       " 'letter': 884,\n",
       " 'standard': 885,\n",
       " 'let': 886,\n",
       " 'far': 887,\n",
       " 'visa': 888,\n",
       " 'spot': 889,\n",
       " 'overview': 890,\n",
       " 'import': 891,\n",
       " 'attitude': 892,\n",
       " 'sender': 893,\n",
       " 'arrive': 894,\n",
       " 'pre': 895,\n",
       " 'relate': 896,\n",
       " 'lecturer': 897,\n",
       " 'undoubtedly': 898,\n",
       " 'action': 899,\n",
       " 'electronically': 900,\n",
       " 'co': 901,\n",
       " 'thesis': 902,\n",
       " 'le': 903,\n",
       " 'own': 904,\n",
       " 'ticket': 905,\n",
       " 'latin': 906,\n",
       " 'book': 907,\n",
       " 'brief': 908,\n",
       " 'audio': 909,\n",
       " 'version': 910,\n",
       " 'owner': 911,\n",
       " 'quit': 912,\n",
       " 'barbara': 913,\n",
       " 'presentation': 914,\n",
       " 'themselve': 915,\n",
       " 'texa': 916,\n",
       " 'incorporate': 917,\n",
       " 'lawyer': 918,\n",
       " 'current': 919,\n",
       " 'availability': 920,\n",
       " 'launch': 921,\n",
       " 'opportunity': 922,\n",
       " 'discourse': 923,\n",
       " 'exclusive': 924,\n",
       " 'series': 925,\n",
       " 'reflect': 926,\n",
       " 'homepage': 927,\n",
       " 'de': 928,\n",
       " 'sign': 929,\n",
       " 'principle': 930,\n",
       " 'effective': 931,\n",
       " 'reality': 932,\n",
       " 'postage': 933,\n",
       " 'genuine': 934,\n",
       " 'path': 935,\n",
       " 'partner': 936,\n",
       " 'particle': 937,\n",
       " 'scientist': 938,\n",
       " 'experience': 939,\n",
       " 'solution': 940,\n",
       " 'bring': 941,\n",
       " 'reconstruction': 942,\n",
       " 'formation': 943,\n",
       " 'spam': 944,\n",
       " 'vendor': 945,\n",
       " 'semantics': 946,\n",
       " 'comparison': 947,\n",
       " 'common': 948,\n",
       " 'sex': 949,\n",
       " 'sum': 950,\n",
       " 'due': 951,\n",
       " 'association': 952,\n",
       " 'everything': 953,\n",
       " 'clearly': 954,\n",
       " 'contribute': 955,\n",
       " 'mailto': 956,\n",
       " 'tilburg': 957,\n",
       " 'nlg': 958,\n",
       " 'two': 959,\n",
       " 'anon': 960,\n",
       " 'shock': 961,\n",
       " 'refund': 962,\n",
       " 'traditional': 963,\n",
       " 'analyze': 964,\n",
       " 'strategy': 965,\n",
       " 'culture': 966,\n",
       " 'parent': 967,\n",
       " 'financially': 968,\n",
       " 'campus': 969,\n",
       " 'cheque': 970,\n",
       " 'demonstrate': 971,\n",
       " 'verbal': 972,\n",
       " 'million': 973,\n",
       " 'attempt': 974,\n",
       " 'characteristic': 975,\n",
       " 'exercise': 976,\n",
       " 'discuss': 977,\n",
       " 'totally': 978,\n",
       " 'suitable': 979,\n",
       " 'main': 980,\n",
       " 'effect': 981,\n",
       " 'hr': 982,\n",
       " 'cognition': 983,\n",
       " 'sources': 984,\n",
       " 'literary': 985,\n",
       " 'center': 986,\n",
       " 'welcome': 987,\n",
       " 'spring': 988,\n",
       " 'implementation': 989,\n",
       " 'international': 990,\n",
       " 'technology': 991,\n",
       " 'voice': 992,\n",
       " 'philosophy': 993,\n",
       " 'remember': 994,\n",
       " 'condition': 995,\n",
       " 'together': 996,\n",
       " 'mine': 997,\n",
       " 'santa': 998,\n",
       " 'guideline': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Index_Dict = { w: i for i, w in enumerate(Common_Words) }\n",
    "Index_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{transform_to_vector}(L)$ takes a list of words $L$ and transforms this list into a vector $\\mathbf{v}$.  If \n",
    "$\\texttt{CommonWords}[i] = w$, then $\\mathbf{v}[i]$ specifies the number of times that $w$ occurs in $L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vector(L):\n",
    "    Result = np.zeros((len(Common_Words, )))\n",
    "    for w in L:\n",
    "        if w in Index_Dict:\n",
    "            Result[Index_Dict[w]] += 1\n",
    "    return Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{get_word_vector}(fn)$ takes a filename `fn`, reads the specified file and transforms it into a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(fn):\n",
    "    with open(fn) as file:\n",
    "        text = file.read()\n",
    "        text = text.lower()\n",
    "        return transform_to_vector(re.findall(r\"[\\w']+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, the notion <em style='color:blue;'>term</em> is used as a synonym for <em style='color:blue;'>word</em>.\n",
    "Given a term $t$ and a document $d$, the <em style='color:blue;'>term frequency</em> $\\texttt{tf}(t, d)$ is defined as\n",
    "$$ \\texttt{tf}(t, d) = \\frac{d.\\texttt{count}(t)}{\\texttt{len}(d)}, $$\n",
    "where $d.\\texttt{count}(t)$ counts the number of times $t$ appears in $d$ and $\\texttt{len}(d)$ is the length of the list representing $d$.\n",
    "\n",
    "A <em style='color:blue;'>corpus</em> is a set of documents.  Given a term $t$ and a corpus $\\mathcal{C}$, the <em style='color:blue;'>inverse document frequency</em> \n",
    "$\\texttt{idf}(t,\\mathcal{C})$ is defined as\n",
    "$$ \\texttt{idf}(t,\\mathcal{C}) = \\ln\\left(\\frac{\\texttt{card}(\\mathcal{C}) + 1}{\\texttt{card}\\bigl(\\{ d \\in \\mathcal{C} \\mid t \\in d \\}\\bigr) + 1}\\right). $$\n",
    "The addition of $1$ in both nominator and denominator is called <em style=\"color:blue;\">Laplace smoothing</em>.  This is necessary to prevent a **division by zero** error \n",
    "for those terms $t$ that do not occur in the list `Common_Words`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute the Feature Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{feature_matrix}(\\texttt{spam_dir}, \\texttt{ham_dir})$ takes two directories that contain spam and ham, respectively.\n",
    "It computes a matrix $X$ and a vector $Y$, where $X$ is the feature matrix and for\n",
    "every row $r$ of the feature matrix, $Y[r]$ is 1 if the mail is ham and 0 if it's spam.\n",
    "\n",
    "The way $X$ is computed is quite inefficient, it would have been better to initialize $X$ as a matrix with the shape $(N,M)$, where $N$ is the number of mails and $M$ is the number of common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix(spam_dir, ham_dir):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for fn in os.listdir(spam_dir):\n",
    "        X.append(get_word_vector(spam_dir + fn))\n",
    "        Y.append(0)\n",
    "    for fn in os.listdir(ham_dir):    \n",
    "        X.append(get_word_vector(ham_dir + fn))\n",
    "        Y.append(+1)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the training set into a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 20.7 ms, total: 184 ms\n",
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, Y_train = feature_matrix(spam_dir_train, ham__dir_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, the feature matrix contains only the term frequencies.  Next we multiply with the inverse document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, _ = X_train.shape\n",
    "IDF  = {}\n",
    "for w, i in Index_Dict.items():\n",
    "    IDF[w] = np.log((N + 1) / (Word_Counter[w] + 1))\n",
    "    X_train[:, i] = X_train[:, i] * IDF[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the feature matrix for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = feature_matrix(spam_dir_test, ham__dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in Index_Dict.items():\n",
    "    X_test[:, i] = X_test[:, i] * IDF[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train and Test a Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an SVM and compute the accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = svm.SVC(kernel='linear', C=100000)\n",
    "M.fit  (X_train, Y_train)\n",
    "M.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884615384615385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
